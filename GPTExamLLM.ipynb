{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6b0TgKIRwEBwAg4RNO3X3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/propenster/GPTExamLLM/blob/main/GPTExamLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Welcome to ExamLLM**\n",
        "ExamLLM is a large language model to generate exam/mock questions and multiple choice answers from any corpus provided in prompt."
      ],
      "metadata": {
        "id": "pJ-qMCJb0BhV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CodeSection - models.py**"
      ],
      "metadata": {
        "id": "Dbpyrrm60XIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fTvCfLroKf3O",
        "outputId": "188f91cd-e2a0-4185-dea4-60ccbde7b648"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.30.0\n",
            "    Uninstalling transformers-4.30.0:\n",
            "      Successfully uninstalled transformers-4.30.0\n",
            "Successfully installed tokenizers-0.15.0 transformers-4.35.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Fine-tuning the library models for examination on a text file (GPT, GPT-2).\n",
        "We fine-tuned GPT-2 to generate multiple-choice type questions and answer from any text corpus\n",
        ".\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "train_input_file = \"sample_data/train_gptexam.txt\"\n",
        "out_dir = \"./out_model/fine-tuned-gptexam-model\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "#eos_token = '\\n'\n",
        "\n",
        "# Customize the tokenizer with the new end-of-stream token\n",
        "#special_tokens_dict = {'eos_token': eos_token}\n",
        "#num_added_tokens = tokenizer.add_tokens(eos_token, special_tokens=True)\n",
        "\n",
        "# Resize model embeddings to match the new tokenizer\n",
        "#model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "\n",
        "\n",
        "# loading and preprocessing tokenized train data\n",
        "train_dataset = TextDataset(\n",
        "    tokenizer = tokenizer,\n",
        "    file_path = train_input_file,\n",
        "    block_size = 128\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer = tokenizer,\n",
        "    mlm = False\n",
        ")\n",
        "\n",
        "#setup trainig args\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = out_dir,\n",
        "    overwrite_output_dir = True,\n",
        "    num_train_epochs = 3,\n",
        "    per_device_train_batch_size = 4,\n",
        "    save_steps = 10_000,\n",
        "    save_total_limit = 2\n",
        ")\n",
        "\n",
        "# set trainer and fine_tune\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = train_dataset,\n",
        ")\n",
        "\n",
        "#train\n",
        "trainer.train()\n",
        "\n",
        "#save the fine-tuned model to outdir\n",
        "model.save_pretrained(out_dir)\n",
        "tokenizer.save_pretrained(out_dir)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "3co36I-Z0jux",
        "outputId": "0df5be19-7b36-4d1c-ca1a-175bf7a0483f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:07, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./out_model/fine-tuned-gptexam-model/tokenizer_config.json',\n",
              " './out_model/fine-tuned-gptexam-model/special_tokens_map.json',\n",
              " './out_model/fine-tuned-gptexam-model/vocab.json',\n",
              " './out_model/fine-tuned-gptexam-model/merges.txt',\n",
              " './out_model/fine-tuned-gptexam-model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our Evaluator** - RunGen.py"
      ],
      "metadata": {
        "id": "D-_TZZ14Po_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import random\n",
        "\n",
        "\n",
        "def generate_multiple_choice_questions(input_text, model, tokenizer, num_questions=3):\n",
        "  #tokenize prompt\n",
        "  input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "  eos_token_id = model.config.eos_token_id\n",
        "\n",
        "  print(f\"eos_token_id: {eos_token_id}\")\n",
        "  # Generate\n",
        "  output = model.generate(\n",
        "      input_ids,\n",
        "      max_length=len(input_ids[0]) + 50,\n",
        "      num_beams = 5,\n",
        "      num_return_sequences = num_questions,\n",
        "      no_repeat_ngram_size = 2,\n",
        "      pad_token_id = eos_token_id,\n",
        "      attention_mask = input_ids != eos_token_id,\n",
        "  )\n",
        "\n",
        "  # decode and return\n",
        "  generated_question = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "  # Generate options (a), (b), (c) randomly\n",
        "  #options = [f\"({chr(97 + i)}) {tokenizer.decode(model.generate(input_ids, max_length=20)[0], skip_special_tokens=True)}\" for i in range(3)]\n",
        "\n",
        "  # Randomly choose one option as the correct answer\n",
        "  correct_answer = \"(b) \" if \"b)\" in generated_question else \"(a) \"\n",
        "\n",
        "  # Generate options (a), (b), (c)\n",
        "  options = [\n",
        "        f\"(a) {tokenizer.decode(model.generate(input_ids, max_length=20)[0], skip_special_tokens=True)}\",\n",
        "        f\"(b) {tokenizer.decode(model.generate(input_ids, max_length=20)[0], skip_special_tokens=True)}\",\n",
        "        f\"(c) {tokenizer.decode(model.generate(input_ids, max_length=20)[0], skip_special_tokens=True)}\"\n",
        "    ]\n",
        "\n",
        "  return generated_question, options, correct_answer\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load fine-tuned model and tokenizer\n",
        "    model_path = \"./out_model/fine-tuned-gptexam-model\"\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "    # Example input text\n",
        "    input_text = \"The organelle responsible for energy or power generation in the cell is the mitochondria. The greenhouse effect is a natural process that warms the Earth's surface. Ribosomes are located in the cytoplasm of the cell and they help me with protein synthesis.\"\n",
        "\n",
        "    # Use our fine-tuned model to generate multiple-choice questions\n",
        "\n",
        "    generated_question, options, correct_answer = generate_multiple_choice_questions(input_text, model, tokenizer)\n",
        "\n",
        "    # Print the generated question, options, and correct answer\n",
        "    print(f\"Generated Question: {generated_question}\")\n",
        "    print(f\"Options: {', '.join(options)}\")\n",
        "    print(f\"Correct Answer: {correct_answer}\")\n",
        "\n",
        "    # # Print the generated questions\n",
        "    # for i, question in enumerate(generated_questions, 1):\n",
        "    #     print(f\"Question {i}: {question}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJsSJHoGLKJJ",
        "outputId": "4e2e05c8-ad9c-4fd9-b833-6e0f9a5d167c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eos_token_id: 50256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 54, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Question: The organelle responsible for energy or power generation in the cell is the mitochondria. The greenhouse effect is a natural process that warms the Earth's surface. Ribosomes are located in the cytoplasm of the cell and they help me with protein synthesis.\n",
            "\n",
            "Ribosome formation is an important part of cell function. It is important for the formation of proteins and nucleic acids that are needed for cell division and cell growth. In the body, the ribosomal protein, which is responsible\n",
            "Options: (a) The organelle responsible for energy or power generation in the cell is the mitochondria. The greenhouse effect is a natural process that warms the Earth's surface. Ribosomes are located in the cytoplasm of the cell and they help me with protein synthesis. The, (b) The organelle responsible for energy or power generation in the cell is the mitochondria. The greenhouse effect is a natural process that warms the Earth's surface. Ribosomes are located in the cytoplasm of the cell and they help me with protein synthesis. The, (c) The organelle responsible for energy or power generation in the cell is the mitochondria. The greenhouse effect is a natural process that warms the Earth's surface. Ribosomes are located in the cytoplasm of the cell and they help me with protein synthesis. The\n",
            "Correct Answer: (a) \n"
          ]
        }
      ]
    }
  ]
}